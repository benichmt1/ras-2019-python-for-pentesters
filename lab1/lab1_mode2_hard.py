#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#  lab1.py
#  
################################################################################
# Lab 1
# =====
# Build a combination directory-bruteforcer / web crawler to enumerate a given
# URL. Third party libraries in use include requests for making the HTTP
# requests, along with requests-html to parse the HTML for links.
#
# **NOTE:** The requests-html dependency makes this script depend on Python 3.6+.
#
# Core Concepts
# -------------
# * Script structure
# * Making HTTP requests
# * Flow control (i.e. loops and exception handling)
#
# Bonus Challenges
# ----------------
# 1) Update the script to take multiple target URLs from the CLI instead of one.
# 2) Check the HTTP responses for vulnerabilities like missing / misconfigured headers.
#

import argparse
import collections
import sys
import urllib.parse
import urllib3

import requests
import requests_html

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_url_authority(url):
    parsed = urllib.parse.urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        return None
    return parsed.scheme + '://' + parsed.netloc

def main():
    # setup the arguments to consume from the command line
    parser = argparse.ArgumentParser(description='bruteforce web directories')
    # todo: you're in hard mode now, write the script your self
    return 0

if __name__ == '__main__':
    sys.exit(main())
